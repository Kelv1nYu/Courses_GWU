{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Handling-the-Identifiers\" data-toc-modified-id=\"Handling-the-Identifiers-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Handling the Identifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combining-the-training-and-testing-data\" data-toc-modified-id=\"Combining-the-training-and-testing-data-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Combining the training and testing data</a></span></li><li><span><a href=\"#Identifying-the-identifiers\" data-toc-modified-id=\"Identifying-the-identifiers-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Identifying the identifiers</a></span></li><li><span><a href=\"#Removing-the-Identifiers\" data-toc-modified-id=\"Removing-the-Identifiers-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Removing the Identifiers</a></span></li></ul></li><li><span><a href=\"#Handling-Uncommon-Variables\" data-toc-modified-id=\"Handling-Uncommon-Variables-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Handling Uncommon Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identifying-uncommon-variables\" data-toc-modified-id=\"Identifying-uncommon-variables-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Identifying uncommon variables</a></span></li><li><span><a href=\"#Removing-uncommon-variables\" data-toc-modified-id=\"Removing-uncommon-variables-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Removing uncommon variables</a></span></li></ul></li><li><span><a href=\"#Handling-date-time-variables\" data-toc-modified-id=\"Handling-date-time-variables-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Handling date time variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-date-time-variables\" data-toc-modified-id=\"Transforming-date-time-variables-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Transforming date time variables</a></span></li></ul></li><li><span><a href=\"#Handling-Missing-Data\" data-toc-modified-id=\"Handling-Missing-Data-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Handling Missing Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combining-the-training-and-testing-data\" data-toc-modified-id=\"Combining-the-training-and-testing-data-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Combining the training and testing data</a></span></li><li><span><a href=\"#Identifying-missing-values\" data-toc-modified-id=\"Identifying-missing-values-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Identifying missing values</a></span></li><li><span><a href=\"#Removing-missing-values\" data-toc-modified-id=\"Removing-missing-values-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Removing missing values</a></span></li></ul></li><li><span><a href=\"#Encoding-the-data\" data-toc-modified-id=\"Encoding-the-data-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Encoding the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combining-the-training-and-testing-data\" data-toc-modified-id=\"Combining-the-training-and-testing-data-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Combining the training and testing data</a></span></li><li><span><a href=\"#Identifying-the-Categorical-Variables\" data-toc-modified-id=\"Identifying-the-Categorical-Variables-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Identifying the Categorical Variables</a></span></li><li><span><a href=\"#Encoding-the-features\" data-toc-modified-id=\"Encoding-the-features-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>Encoding the features</a></span></li><li><span><a href=\"#Separating-the-training-and-testing-data\" data-toc-modified-id=\"Separating-the-training-and-testing-data-2.6.4\"><span class=\"toc-item-num\">2.6.4&nbsp;&nbsp;</span>Separating the training and testing data</a></span></li></ul></li><li><span><a href=\"#Scaling-the-data\" data-toc-modified-id=\"Scaling-the-data-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Scaling the data</a></span></li></ul></li><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-feature-matrix-and-target-vector\" data-toc-modified-id=\"Getting-the-feature-matrix-and-target-vector-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Getting the feature matrix and target vector</a></span></li><li><span><a href=\"#Training-the-model\" data-toc-modified-id=\"Training-the-model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Training the model</a></span></li><li><span><a href=\"#Evaluating-the-model\" data-toc-modified-id=\"Evaluating-the-model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Evaluating the model</a></span></li><li><span><a href=\"#Generating-the-Prediction\" data-toc-modified-id=\"Generating-the-Prediction-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Generating the Prediction</a></span></li><li><span><a href=\"#Generating-the-Submission-File\" data-toc-modified-id=\"Generating-the-Submission-File-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Generating the Submission File</a></span></li></ul></li><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Discussion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Machine Learning I (DATS 6202), Spring 2020\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Homework 1\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Author: Yuxiao Huang\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Here we will work on kaggle competation [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration/data)\n",
    "- The goal of this homework:\n",
    "    - practice data preprocessing and linear regression discussed in [Case_Study_1](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/spring_2020/case_study/case_study_1/case_study_1.ipynb)\n",
    "    - learn two new data preprocessing approaches to handle\n",
    "        - uncommon variables\n",
    "        - date time variables\n",
    "- Complete the missing parts indicated by # Implement me\n",
    "- Particularly, the code should\n",
    "    - be bug-free (note that the output produced by your solution may not necessarily be the same as the provided output, due to version issues)\n",
    "    - be commented\n",
    "- **Marks will be deducted if the above requirements (for the code) are not met**\n",
    "- Submit an ipynb file named homework_1.ipynb to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458644, 11)\n",
      "(625134, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Implement me\n",
    "# Load the raw training data (train.csv)\n",
    "df_raw_train = pd.read_csv('train.csv')\n",
    "# Make a copy of df_raw_train\n",
    "df_train = df_raw_train.copy(deep=True)\n",
    "\n",
    "# Implement me\n",
    "# Load the raw testing data (test.csv)\n",
    "df_raw_test = pd.read_csv('test(1).csv')\n",
    "# Make a copy of df_raw_test\n",
    "df_test = df_raw_test.copy(deep=True)\n",
    "\n",
    "# Implement me\n",
    "# Specify the name of the target\n",
    "target = 'trip_duration'\n",
    "\n",
    "# Print the dimension of df_train and df_test\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0801584</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-30 22:01:40</td>\n",
       "      <td>2016-01-30 22:09:03</td>\n",
       "      <td>6</td>\n",
       "      <td>-73.982857</td>\n",
       "      <td>40.742195</td>\n",
       "      <td>-73.992081</td>\n",
       "      <td>40.749184</td>\n",
       "      <td>N</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1813257</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-17 22:34:59</td>\n",
       "      <td>2016-06-17 22:40:40</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.969017</td>\n",
       "      <td>40.757839</td>\n",
       "      <td>-73.957405</td>\n",
       "      <td>40.765896</td>\n",
       "      <td>N</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id1324603</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-21 07:54:58</td>\n",
       "      <td>2016-05-21 08:20:49</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.969276</td>\n",
       "      <td>40.797779</td>\n",
       "      <td>-73.922470</td>\n",
       "      <td>40.760559</td>\n",
       "      <td>N</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id1301050</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-27 23:12:23</td>\n",
       "      <td>2016-05-27 23:16:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.738400</td>\n",
       "      <td>-73.985786</td>\n",
       "      <td>40.732815</td>\n",
       "      <td>N</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id0012891</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 21:45:01</td>\n",
       "      <td>2016-03-10 22:05:26</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981049</td>\n",
       "      <td>40.744339</td>\n",
       "      <td>-73.973000</td>\n",
       "      <td>40.789989</td>\n",
       "      <td>N</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "5  id0801584          2  2016-01-30 22:01:40  2016-01-30 22:09:03   \n",
       "6  id1813257          1  2016-06-17 22:34:59  2016-06-17 22:40:40   \n",
       "7  id1324603          2  2016-05-21 07:54:58  2016-05-21 08:20:49   \n",
       "8  id1301050          1  2016-05-27 23:12:23  2016-05-27 23:16:38   \n",
       "9  id0012891          2  2016-03-10 21:45:01  2016-03-10 22:05:26   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "5                6        -73.982857        40.742195         -73.992081   \n",
       "6                4        -73.969017        40.757839         -73.957405   \n",
       "7                1        -73.969276        40.797779         -73.922470   \n",
       "8                1        -73.999481        40.738400         -73.985786   \n",
       "9                1        -73.981049        40.744339         -73.973000   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  \n",
       "5         40.749184                  N            443  \n",
       "6         40.765896                  N            341  \n",
       "7         40.760559                  N           1551  \n",
       "8         40.732815                  N            255  \n",
       "9         40.789989                  N           1225  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 10 rows of df_train\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3004672</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:58</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.988129</td>\n",
       "      <td>40.732029</td>\n",
       "      <td>-73.990173</td>\n",
       "      <td>40.756680</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3505355</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:53</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.964203</td>\n",
       "      <td>40.679993</td>\n",
       "      <td>-73.959808</td>\n",
       "      <td>40.655403</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1217141</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:47</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.997437</td>\n",
       "      <td>40.737583</td>\n",
       "      <td>-73.986160</td>\n",
       "      <td>40.729523</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id2150126</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:59:41</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.956070</td>\n",
       "      <td>40.771900</td>\n",
       "      <td>-73.986427</td>\n",
       "      <td>40.730469</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1598245</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:33</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.970215</td>\n",
       "      <td>40.761475</td>\n",
       "      <td>-73.961510</td>\n",
       "      <td>40.755890</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0668992</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.991302</td>\n",
       "      <td>40.749798</td>\n",
       "      <td>-73.980515</td>\n",
       "      <td>40.786549</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1765014</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:15</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.978310</td>\n",
       "      <td>40.741550</td>\n",
       "      <td>-73.952072</td>\n",
       "      <td>40.717003</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id0898117</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:09</td>\n",
       "      <td>2</td>\n",
       "      <td>-74.012711</td>\n",
       "      <td>40.701527</td>\n",
       "      <td>-73.986481</td>\n",
       "      <td>40.719509</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id3905224</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:58:55</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.992332</td>\n",
       "      <td>40.730511</td>\n",
       "      <td>-73.875618</td>\n",
       "      <td>40.875214</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id1543102</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:58:46</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.993179</td>\n",
       "      <td>40.748760</td>\n",
       "      <td>-73.979309</td>\n",
       "      <td>40.761311</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime  passenger_count  \\\n",
       "0  id3004672          1  2016-06-30 23:59:58                1   \n",
       "1  id3505355          1  2016-06-30 23:59:53                1   \n",
       "2  id1217141          1  2016-06-30 23:59:47                1   \n",
       "3  id2150126          2  2016-06-30 23:59:41                1   \n",
       "4  id1598245          1  2016-06-30 23:59:33                1   \n",
       "5  id0668992          1  2016-06-30 23:59:30                1   \n",
       "6  id1765014          1  2016-06-30 23:59:15                1   \n",
       "7  id0898117          1  2016-06-30 23:59:09                2   \n",
       "8  id3905224          2  2016-06-30 23:58:55                2   \n",
       "9  id1543102          2  2016-06-30 23:58:46                1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.988129        40.732029         -73.990173         40.756680   \n",
       "1        -73.964203        40.679993         -73.959808         40.655403   \n",
       "2        -73.997437        40.737583         -73.986160         40.729523   \n",
       "3        -73.956070        40.771900         -73.986427         40.730469   \n",
       "4        -73.970215        40.761475         -73.961510         40.755890   \n",
       "5        -73.991302        40.749798         -73.980515         40.786549   \n",
       "6        -73.978310        40.741550         -73.952072         40.717003   \n",
       "7        -74.012711        40.701527         -73.986481         40.719509   \n",
       "8        -73.992332        40.730511         -73.875618         40.875214   \n",
       "9        -73.993179        40.748760         -73.979309         40.761311   \n",
       "\n",
       "  store_and_fwd_flag  \n",
       "0                  N  \n",
       "1                  N  \n",
       "2                  N  \n",
       "3                  N  \n",
       "4                  N  \n",
       "5                  N  \n",
       "6                  N  \n",
       "7                  N  \n",
       "8                  N  \n",
       "9                  N  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 10 rows of df_test\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Identifiers\n",
    "Reference: the code in this section is largely based on [Case_Study_1](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/spring_2020/case_study/case_study_1/case_study_1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the training and testing data\n",
    "The code below shows how to combine the training and testing data (using pandas concat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine df_train and df_test\n",
    "df = pd.concat([df_train, df_test], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the identifiers\n",
    "\n",
    "The code below shows how to find identifiers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_checker(df):\n",
    "    \"\"\"\n",
    "    The identifier checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the identifiers\n",
    "    df_id = df[[var for var in df.columns \n",
    "                if df[var].nunique(dropna=True) == df[var].notnull().sum()]]\n",
    "                \n",
    "    return df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call id_checker on df\n",
    "df_id = id_checker(df)\n",
    "\n",
    "# Print the first 5 rows of df_id\n",
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the Identifiers\n",
    "The code below shows how to remove the identifiers from data (using pandas DataFrame.drop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implement me\n",
    "# Remove the identifiers from df_train\n",
    "df_train = df_train.drop(columns = df_id.columns)\n",
    "\n",
    "# Implement me\n",
    "# Remove the identifiers from df_test\n",
    "df_test = df_test.drop(columns = df_id.columns)\n",
    "\n",
    "# Print the dimension of df_train and df_test\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the first 10 rows of df_train\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the first 10 rows of df_test\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Uncommon Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying uncommon variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to find common variables between the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def common_var_checker(df_train, df_test, target):\n",
    "    \"\"\"\n",
    "    The common variables checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : dataframe of training data\n",
    "    df_test : dataframe of testing data\n",
    "    target : the name of the target\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of common variables between the training and testing data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of common variables between the training and testing data\n",
    "    df_common_var = pd.DataFrame(np.intersect1d(df_train.columns, np.union1d(df_test.columns, [target])),\n",
    "                                 columns=['common var'])\n",
    "                \n",
    "    return df_common_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call common_var_checker\n",
    "df_common_var = common_var_checker(df_train, df_test, target)\n",
    "\n",
    "# Print df_common_var\n",
    "df_common_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to find the variables in the training data but not in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the variables in the training data but not in the testing data\n",
    "uncommon_var_train_not_test = np.setdiff1d(df_train.columns, df_common_var['common var'])\n",
    "\n",
    "# Print the uncommon variables\n",
    "pd.DataFrame(uncommon_var_train_not_test, columns=['uncommon var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to find the variables in the testing data but not in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the variables in the testing data but not in the training data\n",
    "uncommon_var_test_not_train = np.setdiff1d(df_test.columns, df_common_var['common var'])\n",
    "\n",
    "# Print the uncommon variables\n",
    "pd.DataFrame(uncommon_var_test_not_train, columns=['uncommon var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing uncommon variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove the uncommon variables from the training data\n",
    "df_train = df_train.drop(columns=uncommon_var_train_not_test)\n",
    "\n",
    "# Print the first 10 rows of df_train\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove the uncommon variables from the testing data\n",
    "df_test = df_test.drop(columns=uncommon_var_test_not_train)\n",
    "\n",
    "# Print the first 10 rows of df_test\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling date time variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming date time variables\n",
    "The code below shows how to transform date time variables into the following 6 datetime types: year, month, day, hour, minute and second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_transformer(df, datetime_vars):\n",
    "    \"\"\"\n",
    "    The datetime transformer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    datetime_vars : the datetime variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe where datetime_vars are transformed into the following 6 datetime types:\n",
    "    year, month, day, hour, minute and second\n",
    "    \"\"\"\n",
    "    \n",
    "    # The dictionary with key as datetime type and value as datetime type operator\n",
    "    dict_ = {'year'   : lambda x : x.dt.year,\n",
    "             'month'  : lambda x : x.dt.month,\n",
    "             'day'    : lambda x : x.dt.day,\n",
    "             'hour'   : lambda x : x.dt.hour,\n",
    "             'minute' : lambda x : x.dt.minute,\n",
    "             'second' : lambda x : x.dt.second}\n",
    "    \n",
    "    # Make a copy of df\n",
    "    df_datetime = df.copy(deep=True)\n",
    "    \n",
    "    # For each variable in datetime_vars\n",
    "    for var in datetime_vars:\n",
    "        # Cast the variable to datetime\n",
    "        df_datetime[var] = pd.to_datetime(df_datetime[var])\n",
    "        \n",
    "        # For each item (datetime_type and datetime_type_operator) in dict_\n",
    "        for datetime_type, datetime_type_operator in dict_.items():\n",
    "            # Add a new variable to df_datetime where:\n",
    "            # the variable's name is var + '_' + datetime_type\n",
    "            # the variable's values are the ones obtained by datetime_type_operator\n",
    "            df_datetime[var + '_' + datetime_type] = datetime_type_operator(df_datetime[var])\n",
    "            \n",
    "    # Remove datetime_vars from df_datetime\n",
    "    df_datetime = df_datetime.drop(columns=datetime_vars)\n",
    "                \n",
    "    return df_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call datetime_transformer on df_train\n",
    "df_train = datetime_transformer(df_train, ['pickup_datetime'])\n",
    "\n",
    "# Print the first 10 rows of df_train\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call datetime_transformer on df_test\n",
    "df_test = datetime_transformer(df_test, ['pickup_datetime'])\n",
    "\n",
    "# Print the first 10 rows of df_test\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "Reference: the code in this section is largely based on [Case_Study_1](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/spring_2020/case_study/case_study_1/case_study_1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the training and testing data\n",
    "The code below shows how to combine the training and testing data (using pandas concat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine df_train and df_test\n",
    "df = pd.concat([df_train, df_test], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to find variables with NaN (using pandas DataFrame.isna), their proportion of NaN and dtype (data type objects, using pandas Series.dtype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_checker(df):\n",
    "    \"\"\"\n",
    "    The NaN checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of variables with NaN, their proportion of NaN and dtype\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the variables with NaN, their proportion of NaN and dtype\n",
    "    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n",
    "                           for var in df.columns if df[var].isna().sum() > 0],\n",
    "                          columns=['var', 'proportion', 'dtype'])\n",
    "    \n",
    "    # Sort df_nan in accending order of the proportion of NaN\n",
    "    df_nan = df_nan.sort_values(by='proportion', ascending=False)\n",
    "    \n",
    "    return df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call nan_checker on df\n",
    "df_nan = nan_checker(df)\n",
    "\n",
    "# Print df_nan\n",
    "df_nan.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the unique dtype of the variables with NaN\n",
    "pd.DataFrame(df_nan['dtype'].unique(), columns=['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to use dtype to select variables with missing values in the combined training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the variables with missing values, their proportion of missing values and dtype\n",
    "df_miss = df_nan[df_nan['dtype'] == 'float64']\n",
    "\n",
    "# Print df_miss\n",
    "df_miss.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing missing values\n",
    "The code below shows how to remove rows with missing values (here we use numpy intersect1d to find the intersection of two series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Remove rows with missing values from df_train\n",
    "df_train = df_train.dropna(subset=np.intersect1d(df_miss['var'], \n",
    "                                                 df_train.columns),\n",
    "                           inplace=False)\n",
    "\n",
    "# Remove rows with missing values from df_test\n",
    "df_test = df_test.dropna(subset=np.intersect1d(df_miss['var'], \n",
    "                                               df_test.columns),\n",
    "                         inplace=False)\n",
    "\n",
    "# Print the dimension of the training and testing data\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data\n",
    "Reference: the code in this section is largely based on [Case_Study_1](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/spring_2020/case_study/case_study_1/case_study_1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the training and testing data\n",
    "The code below shows how to combine the training and testing data (using pandas concat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine df_train and df_test\n",
    "df = pd.concat([df_train, df_test], sort=False)\n",
    "\n",
    "# Print the unique dtype of variables in df\n",
    "pd.DataFrame(df.dtypes.unique(), columns=['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to find the categorical variables that have object as dtype (using pandas.Series.dtype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_var_checker(df):\n",
    "    \"\"\"\n",
    "    The categorical variable checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: the dataframe\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of categorical variables and their number of unique value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of categorical variables and their number of unique value\n",
    "    df_cat = pd.DataFrame([[var, df[var].nunique(dropna=False)]\n",
    "                           for var in df.columns if df[var].dtype == 'object'],\n",
    "                          columns=['var', 'nunique'])\n",
    "    \n",
    "    # Sort df_cat in accending order of the number of unique value\n",
    "    df_cat = df_cat.sort_values(by='nunique', ascending=False)\n",
    "    \n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call cat_var_checker on df\n",
    "df_cat = cat_var_checker(df)\n",
    "\n",
    "# Print the dataframe\n",
    "df_cat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the features\n",
    "The code below shows how to encode the categorical features in the combined training and testing data (using pandas.get\\_dummies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Implement me\n",
    "# One-hot-encode the categorical features in the combined training and testing data\n",
    "df = pd.get_dummies(df, columns = df_cat['var'])\n",
    "\n",
    "# Print the first 10 rows (samples) of df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the training and testing data\n",
    "The code below shows how to separate the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the training data\n",
    "df_train = df.iloc[:df_train.shape[0], :]\n",
    "\n",
    "# Separating the testing data\n",
    "df_test = df.iloc[df_train.shape[0]:, :]\n",
    "\n",
    "# Print the dimension of the training and testing data\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to standardize the data (using sklearn StandardScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# The StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Implement me\n",
    "# Standardize the training data\n",
    "df_train = pd.DataFrame(ss.fit_transform(df_train), columns=df_train.columns)\n",
    "\n",
    "# Implement me\n",
    "# Standardize the testing data\n",
    "df_test = pd.DataFrame(ss.transform(df_test), columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the feature matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature matrix\n",
    "X_train = df_train[np.setdiff1d(df_train.columns, [target])]\n",
    "X_test = df_test[np.setdiff1d(df_test.columns, [target])]\n",
    "\n",
    "# Get the target vector\n",
    "y_train = df_train[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# The LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Implement me\n",
    "# Train the linear regression model on the training data using X_train and y_train\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Implement me\n",
    "# Get the prediction on the training data using lr\n",
    "y_train_pred = lr.predict(X_train)\n",
    "\n",
    "# Get the MSE\n",
    "mean_squared_error(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement me\n",
    "# Get the prediction on the testing data using lr\n",
    "y_test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the dataframe of y_test_pred, which has the same shape as df_train\n",
    "df_y_test_pred = pd.DataFrame(np.tile(y_test_pred, df_train.shape[1]),\n",
    "                              columns=df_train.columns)\n",
    "\n",
    "# Transform df_y_test_pred back to the original scale\n",
    "df_y_test_pred = pd.DataFrame(ss.inverse_transform(df_y_test_pred),\n",
    "                              columns=df_train.columns)\n",
    "\n",
    "# Get the submission dataframe\n",
    "df_submit = pd.DataFrame(np.hstack((df_raw_test[['id']], df_y_test_pred[[target]])),\n",
    "                         columns=['id', target])\n",
    "\n",
    "# Generate the submission file\n",
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Do not feel obligated to submit the submission file to kaggle. It will not give you a high ranking on the leaderboard, which is fine at this moment since it is not the goal for this assignment (as suggested in Overview). We will come back to this assignment later this semester with some feature engineering approaches and more powerful models. Stay tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
